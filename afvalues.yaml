## helm repo add airflow-stable https://airflow-helm.github.io/charts

## helm repo update

## helm install airflow airflow-stable/airflow --namespace airflow --version "8.X.X" --values ./values.yaml

## helm upgrade airflow airflow-stable/airflow --namespace airflow --version "8.X.X" --values ./values.yaml

## helm uninstall airflow --namespace airflow


# NOTE:
# - This is intended to be a `custom-values.yaml` starting point for non-production deployment (like minikube)

# External Dependencies:
# - A PUBLIC git repo for DAGs: https://github.com/USERNAME/REPOSITORY.git
#

###################################
# Airflow - Common Configs
###################################
airflow:
  ## the airflow executor type to use
  ##
  executor: CeleryExecutor

  ## the fernet key used to encrypt the connections in the database
  ##
  fernetKey: "K_XTonQLXGpMPDI8E0axELs-AG71Djaw7B9GWqNbUHg="

  ## environment variables for the web/scheduler/worker Pods (for airflow configs)
  ##
  config:
    # Security
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"

    # DAGS
    AIRFLOW__CORE__LOAD_EXAMPLES: "True"
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "30"
    
    ## Disable noisy "Handling signal: ttou" Gunicorn log messages
    GUNICORN_CMD_ARGS: "--log-level WARNING"

  ## a list of initial users to create
  ##
  users:
    - username: admin
      password: wvmu[ug8gvl
      role: Admin
      email: dacho-c@bangkokkomatsusales.com
      firstName: airflow
      lastName: admin

  usersUpdate: false
  ## a list of initial connections to create
  ##
  ##connections:
    ## see docs: https://airflow.apache.org/docs/apache-airflow-providers-google/stable/connections/gcp.html
    ##- id: my_gcp
      ##type: google_cloud_platform
      ##description: my GCP connection
      ##extra: |-
        ##{ "extra__google_cloud_platform__num_retries": "5" }
  ## a list of initial variables to create
  ##
  variables:
    - key: "environment"
      value: "prod"

  ## a list of initial pools to create
  ##
  pools:
    - name: "pool_1"
      slots: 5
      description: "example pool with 5 slots"
    - name: "pool_2"
      slots: 10
      description: "example pool with 10 slots"

###################################
# Airflow - Scheduler Configs
###################################
scheduler:
  ## the number of scheduler Pods to run
  ##
  replicas: 1

###################################
# Airflow - WebUI Configs
###################################
web:
  ## configs for the Service of the web Pods
  ##
  service:
    type: NodePort
    port: 30900

###################################
# Airflow - Worker Configs
###################################
workers:
  ## if the airflow workers StatefulSet should be deployed
  ##
  enabled: true

  ## the number of workers Pods to run
  ##
  replicas: 1
  ## resource requests/limits for the airflow worker Pods
  ##
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"

  ## configs for the PodDisruptionBudget of the worker StatefulSet
  ##
  podDisruptionBudget:
    ## if a PodDisruptionBudget resource is created for the worker StatefulSet
    ##
    enabled: true

    ## the maximum unavailable pods/percentage for the worker StatefulSet
    ##
    ## NOTE:
    ## - prevents loosing more than 20% of current worker task slots in a voluntary
    ##   disruption
    ##
    maxUnavailable: "20%"

  ## configs for the HorizontalPodAutoscaler of the worker Pods
  ##
  autoscaling:
    enabled: true
    maxReplicas: 3
    metrics:
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
          
###################################
# Airflow - Flower Configs
###################################
flower:
  ## if the Flower UI should be deployed
  ##
  enabled: true

###################################
# Airflow - Logs Configs
###################################
logs:
  ## configs for the logs PVC
  ##
  persistence:
    ## if a persistent volume is mounted at `logs.path`
    ##
    enabled: false

###################################
# Airflow - DAGs Configs
###################################
dags:
  ## configs for the git-sync sidecar (https://github.com/kubernetes/git-sync)
  ##
  gitSync:
    ## if the git-sync sidecar container is enabled
    ##
    enabled: true

    ## the url of the git repo
    ##
    repo: "https://github.com/dacho-c/my-dag.git"

    ## the git branch to check out
    ##
    branch: master

    ## the git revision (tag or hash) to check out
    ##
    revision: HEAD

    ## the number of seconds between syncs
    ##
    syncWait: 60

    ## the name of a pre-created Secret with git http credentials
    ##
    httpSecret: "github-mydags"

    ## the key in `dags.gitSync.httpSecret` with your git username
    ##
    httpSecretUsernameKey: ""

    ## the key in `dags.gitSync.httpSecret` with your git password/token
    ##
    httpSecretPasswordKey: ""

###################################
# Database - PostgreSQL Chart
###################################
postgresql:
  ## if the `stable/postgresql` chart is used
  ##
  enabled: false

###################################
# Database - External Database
###################################
externalDatabase:
  ## the type of external database: {mysql,postgres}
  ##
  type: postgres

  ## the host of the external database
  ##
  host: admin.komatsuthailand.com

  ## the port of the external database
  ##
  port: 30202

  ## the database/scheme to use within the the external database
  ##
  database: airflow

  ## the user of the external database
  ##
  user: postgres

  ## the name of a pre-created secret containing the external database password
  ##
  passwordSecret: airflow-postgres-pwd

  ## the key within `externalDatabase.passwordSecret` containing the password string
  ##
  passwordSecretKey: postgres-pwd
###################################
# Database - Redis Chart
###################################
redis:
  enabled: true
